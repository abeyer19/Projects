version 1.0.0 - Beta
    --= 4/23/2025 - 21:38 =--
        Finished setting up the basic structure of the project and writing out the vision/ideas.
        
        Items Added:
        - read_me.txt -> used for basic overview of the project
        - patch_notes.txt -> used to keep track of updates and document progress of the project
        - tinker.txt -> used for recording ideas and basic structure of the project

    --= 4/28/2025 - 21:11  =--
        Finalized setting up API key on local disk space using the .gitignore (committed) and config.env (local only) files. 
        Tested API and printed out sample on tinker.py.
        Added in notes for this on the 'read_me.txt' file for others to copy and easily use without showing API key publicly.

        Items Added:
        - .gitignore
        - config.env (local only)

    --= 4/29/2025 - 21:26 =--
        Updated main.py with tinkerings from tinker.py with a method to search by channels.

        Items Added:
        None

    --= 4/30/2025 - 21:01 =--
        Added in a search.py file to start seperating out tasks. Able to put in search perameters such as query, type, and output limit for more customizable searches. To be used later in production once direction is given towards what data to pull daily in cron.
        Updated main.py to stage new files for searches, channels, videos, etc.
        Updated tinkering to start looking into categories, was able to find category ID's for a new search perameter in search.py.

        Items Added:
        - search.py

    --= 5/2/2025 - 18:27 =---
        Added in an api_connection file for api connection call and a categories file for pulling current YouTube categories for database table.
        Refined the search function to be dynamic between type - either video or channel, specifying datatypes and allowing for some arguments that are passed through the function to be ignored when needed.
        Added in a collector_video.py file to seperate the data collection outside of search.py, which only includes basic search features for metadata such as title and id of videos and channels.
        Added in collector_channel.py file to seperate the data collection outside of search.py, which only includes basic search features for metadata such as title and id of videos and channels.

        Items Added:
        - api_connection.py
        - categories.py
        - collector_video
        - collector_channel

    --= 5/28/2025 - 20:15 =--
        Been on vacation for the past two weeks with no computer access.
        Working on developing out the database in PGAdmin4 and understanding out to use a Python library of my choice to connect to PostgreSQL server and do the ETL process.
        Spent 4 hours trying to uninstall and reinstall PGAdmin4 to start from scratch, will need to find a new PostgreSQL server host or create a new user on my computer for projects.

    --= 6/1/2025 17:42 =--
        Finalized the database creation, but needed to create a new user on my Mac then download PGAdmin4 through Homebrew. I could not find out where the PG conf file was with the password for the application, therefore I         could not replace my password for the admin of PGAdmin4 and create new databases.

    --= 6/3/2025 21:34 =--
        Updated documentation and added more folder file structure.
        Added config.env file so it is already downloaded when copying repository.

        Items Added:
        - config.env

    --= 6/4/2025 21:47 =--
        Finalized how to get the API connection from api_connection.py to work for the entire project. On top of this, spent over 4 hours trying to figure out why I can't import functions from folder to folder when downloaded on my local drive. Need to fix this.
        Set up the first table in PGAdmin4 for the categories data.

    --= 6/5/2025 21:14 =--
        Was able to fix the issue with the data not pulling across folder structures using some work-around I don't really like I found on StackOverflow. Want to refine this.
        Need to start finalizing the pipelines into PGAdmin4 and set up all databases on the server and set up the cronjob to run daily.
        Will upload all files tomorrow from my desktop to github.

    --= 6/8/2025 16:04 =--
        Updated all the work I did over the past couple of days, with some minor tweaks.
        Am able to pull from DBS1, need to finish fully search criteria tonight and going through more tests.
        Ran into unexpected errors when using the search.py function I created, need to understand how to search for videos by category and get results populated. Next few days will be spent fixing this while also setting up the rest of the database tables and finding a dashboarding tool to use.

    --= 6/9/2025 21:24 =--
        Updated search.py in Scrapers to include datetime for logging search information.
        Also created a new data table in PGAdmin4.
        Need to figure out linking the searches and data pushes from the Writers for one seamless main.py file based on search perameters
    
    --= 6/10/2025 20:13 =--
        Updated a lot on the main.py for the Writers.
        Also added new functionality to search and video_collector to work better with the searching functions.
        I want to fix the search and collectors to be able to accept lists of inputs and output a dataframe so I don't have to iterate through lists of information, put into a list, then reformate as a pandas dataframe.
        
        -= 21:09 =-
        Fixed the write_categories to fix duplicate loading, will only need the cron job for this to run once a month or longer incase YouTube changes/adds more categories.

    --= 6/11/2025 21:47 =--
        Updated the dumbest error (referencing the wrong variable) in the main.py file for 'Writers', and in doing so while testing EVERY SINGLE FUNCTION THAT LEADS INTO IT, I wasted all my quota for the day.
            - Learning from this is simply -- look at your code before running through each step with tests, especially if it is Cloud Based API usage that will cost you at a enterprise level.
        Cleaned up a lot of the other python files as well to simplify the pipeline and got rid of testing scripts.
        Spent over 2 hours working through Docker, only to have Superset completely break during download, going to switch to Google Looker if applicable to what I am doing.
        
        Items Added:
        - pully.py

