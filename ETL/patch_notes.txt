version 1.0.0 - Beta
    --= 4/23/2025 - 21:38 =--
        Finished setting up the basic structure of the project and writing out the vision/ideas.
        
        Items Added:
        - read_me.txt -> used for basic overview of the project
        - patch_notes.txt -> used to keep track of updates and document progress of the project
        - tinker.txt -> used for recording ideas and basic structure of the project

    --= 4/28/2025 - 21:11  =--
        Finalized setting up API key on local disk space using the .gitignore (committed) and config.env (local only) files. 
        Tested API and printed out sample on tinker.py.
        Added in notes for this on the 'read_me.txt' file for others to copy and easily use without showing API key publicly.

        Items Added:
        - .gitignore
        - config.env (local only)

    --= 4/29/2025 - 21:26 =--
        Updated main.py with tinkerings from tinker.py with a method to search by channels.

        Items Added:
        None

    --= 4/30/2025 - 21:01 =--
        Added in a search.py file to start seperating out tasks. Able to put in search perameters such as query, type, and output limit for more customizable searches. To be used later in production once direction is given towards what data to pull daily in cron.
        Updated main.py to stage new files for searches, channels, videos, etc.
        Updated tinkering to start looking into categories, was able to find category ID's for a new search perameter in search.py.

        Items Added:
        - search.py

    --= 5/2/2025 - 18:27 =---
        Added in an api_connection file for api connection call and a categories file for pulling current YouTube categories for database table.
        Refined the search function to be dynamic between type - either video or channel, specifying datatypes and allowing for some arguments that are passed through the function to be ignored when needed.
        Added in a collector_video.py file to seperate the data collection outside of search.py, which only includes basic search features for metadata such as title and id of videos and channels.
        Added in collector_channel.py file to seperate the data collection outside of search.py, which only includes basic search features for metadata such as title and id of videos and channels.

        Items Added:
        - api_connection.py
        - categories.py
        - collector_video
        - collector_channel

    --= 5/28/2025 - 20:15 =--
        Been on vacation for the past two weeks with no computer access.
        Working on developing out the database in PGAdmin4 and understanding out to use a Python library of my choice to connect to PostgreSQL server and do the ETL process.
        Spent 4 hours trying to uninstall and reinstall PGAdmin4 to start from scratch, will need to find a new PostgreSQL server host or create a new user on my computer for projects.

    --= 6/1/2025 17:42 =--
        Finalized the database creation, but needed to create a new user on my Mac then download PGAdmin4 through Homebrew. I could not find out where the PG conf file was with the password for the application, therefore I         could not replace my password for the admin of PGAdmin4 and create new databases.

    --= 6/3/2025 21:34 =--
        Updated documentation and added more folder file structure.
        Added config.env file so it is already downloaded when copying repository.

        Items Added:
        - config.env

    --= 6/4/2025 21:47 =--
        Finalized how to get the API connection from api_connection.py to work for the entire project. On top of this, spent over 4 hours trying to figure out why I can't import functions from folder to folder when downloaded on my local drive. Need to fix this.
        Set up the first table in PGAdmin4 for the categories data.

    --= 6/5/2025 21:14 =--
        Was able to fix the issue with the data not pulling across folder structures using some work-around I don't really like I found on StackOverflow. Want to refine this.
        Need to start finalizing the pipelines into PGAdmin4 and set up all databases on the server and set up the cronjob to run daily.
        Will upload all files tomorrow from my desktop to github.

    --= 6/8/2025 16:04 =--
        Updated all the work I did over the past couple of days, with some minor tweaks.
        Am able to pull from DBS1, need to finish fully search criteria tonight and going through more tests.
        Ran into unexpected errors when using the search.py function I created, need to understand how to search for videos by category and get results populated. Next few days will be spent fixing this while also setting up the rest of the database tables and finding a dashboarding tool to use.

    --= 6/9/2025 21:24 =--
        Updated search.py in Scrapers to include datetime for logging search information.
        Also created a new data table in PGAdmin4.
        Need to figure out linking the searches and data pushes from the Writers for one seamless main.py file based on search perameters
    
    --= 6/10/2025 20:13 =--
        Updated a lot on the main.py for the Writers.
        Also added new functionality to search and video_collector to work better with the searching functions.
        I want to fix the search and collectors to be able to accept lists of inputs and output a dataframe so I don't have to iterate through lists of information, put into a list, then reformate as a pandas dataframe.
        
        -= 21:09 =-
        Fixed the write_categories to fix duplicate loading, will only need the cron job for this to run once a month or longer incase YouTube changes/adds more categories.

    --= 6/11/2025 21:47 =--
        Updated the dumbest error (referencing the wrong variable) in the main.py file for 'Writers', and in doing so while testing EVERY SINGLE FUNCTION THAT LEADS INTO IT, I wasted all my quota for the day.
            - Learning from this is simply -- look at your code before running through each step with tests, especially if it is Cloud Based API usage that will cost you at a enterprise level.
        Cleaned up a lot of the other python files as well to simplify the pipeline and got rid of testing scripts.
        Spent over 2 hours working through Docker, only to have Superset completely break during download, going to switch to Google Looker if applicable to what I am doing.
        
        Items Added:
        - pully.py

    --= 6/12/20:10 =--
        Finalized all the issues within the main.py Writers folder and loaded the first data pull into PGAdmin4.
        Downloaded Metabase and started messing around with the dashboarding tool (it uses PostgreSQL which is nice for making customized tables and graphs).

    --= 6/15/2025 13:16 =--
        Started to run the main.py file twice a day and have gotten good results with no errors when pushing to DBS1.
        No updates to production code, just letting the data accumulate while I work through other issues with the dashboarding tools.

    --= 6/16/2025 18:30 =--
        Created the cronjob to run at 8AM and 6PM daily.
        
        -= 21:01 =-
        Updated code to run 10 items instead of 1, to speed up data collection process. Will adjust once I see what the daily quota usage is and if I have enough space.

    --= 6/17/2025 21:24 =--
        Updated pully.py to be customizable and functional.
        Updated quota limit pulls on main.py in Writers.
        Experimented with PGAdmin4 PostgreSQL queries -- will want to research functions as well as triggers to automatically update DBP1 database with queried data for dashboarding.

    --= 6/18/2025 12:52 =--
        Found an error with main.py categories_filter list, where search.py needs a string type for category_id argument in function input. -- need to test more tomorrow when I have quota back.
        Updated pully.py to be more dynamic based on what table was given in the arguments of the function.
        Started to work on dashboarding tables calculations in Lifters

    --= 6/19/2025 19:07 =--
        Troubleshot the errors with the search function returning no video id's, but need to finalize as my quota quickly ran out yesterday. Most likely having too many perameters being passed to the .search().list() on Googles API.

    --= 6/20/2025 14:07 =--
        Started to work on the Medium article and added more to the documentation read_me.txt file on GitHub.

    --= 6/23/2025 =--
        Worked on getting the article written and need to finish the rest of the project first.

    --= 6/24/2025 =--
        Worked on getting the veiws in PGAdmin4 up and running, then started to play around with Metabase (not really that good in my opinion and lacks basic functionality). Will need to play around with the dashboarding tool more.
        Updated search() in writer to return 100 items and not 25, but still can't figure out why some searches pull data and others don't. Found that there really is no consistency with this and you get whatever category you get that is pushed to the database everytime you run it. I can't help but feel that Google does this on purpose, I tested this with the rest of my quota for the day and rerunning the same exact search() function will result in a different dataframe than what was loaded into DBS1. -- will need to monitor this the rest of the time as well as log data, but really need to start accumulating more data in general.
        I think I would like to build my own dashboarding tool in Java or something if I have time to learn it before MSA, but maybe some day.
        Put some more progress towards the article as well in the background to work in tandem.

    --= 6/28/2025 =--
        Figured out what was wrong with the search() function, there needs to be an argument passed to 'q' as the YouTube API does not support random searches.
        Metabase functionality is terrible and I've been running into so many issues, will most likely just switch to total python using Seaborn for visualizations as they are more dynamic in nature.

    --= 7/1/2025 =--
        Still going to use Metabase since I can't find any better dashboarding tool, but visualizations will be weak sauce.
        Worked on the article more and brainstorming dashboarding ideas.

    --= 7/2/2025 =--
        Writer and subsequent collectors are now finally working and gathering data fully now.
        Finalized the Videos Dashboard, need to work on the Channels Dashboard now.

version 1.1.0 - Beta
    --= 7/12/2025 - 12:40 =--
        Working on getting the writer files to be way more dynamic and useable elsewhere in other similar projects.
        Then will start working on setting up the Star Schema tables in the DBP1 database, this is already begun and development is halted to fix writers first so I don't have to go back and change everything afterwards.

        Items added:
        - writer_2.py

        Side note:
        Going to use this phase to start testing on the tokenization and similarity in text project.
        Things I need to learn and look more into are; tokenization, text imbedding, multidimensional vectorization of the embeddings, algorimths implementation for ML or just creating my own somehow
